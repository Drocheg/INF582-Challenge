{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering process Waldor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In Part 1 we will download data like we do in public_baseline. For this data we will calculate features after.\n",
    "In Part 2 we will start feature engineering - make some functions for calculating some generated features.\n",
    "In Part 3 we write modified feature_engineering function -> we add one new argumnet - w2v.\n",
    "In Part 4 we will do a feature_engineering to have all features we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinar\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk\n",
    "import csv\n",
    "import igraph\n",
    "import math\n",
    "\n",
    "from read_data import *\n",
    "from graph_creation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization like in public_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dinar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dinar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "creating graph\n",
      "graph created\n"
     ]
    }
   ],
   "source": [
    "# ---First Initializations--- #\n",
    "path_to_data = \"../data/\"\n",
    "nltk.download('punkt')  # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# ---Read Data--- #\n",
    "testing_set, training_set, node_info = read_data()\n",
    "IDs = [element[0] for element in node_info]\n",
    "\n",
    "# ---Compute TFIDF vector of each paper--- #\n",
    "corpus = [element[5] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# each row is a node in the order of node_info\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)\n",
    "pairwise_similarity = features_TFIDF * features_TFIDF.T\n",
    "#print pairwise_similarity.shape\n",
    "# ---Create graph--- #\n",
    "g = create_graph(training_set, IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional to public_baseline graph features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_w2v(node_info, stemmer, stpwds):\n",
    "    try:\n",
    "        model = Word2Vec.load(\"w2v_model\")\n",
    "        print (\"Word2Vec model loaded\")\n",
    "    except:\n",
    "        path_to_google_news = '../data'\n",
    "        my_q = 300 # to match dim of GNews word vectors\n",
    "        mcount = 5\n",
    "        model = Word2Vec(size=my_q, min_count=mcount)\n",
    "        cleaned_abstracts = [clean(element[5], stemmer, stpwds) for element in node_info]\n",
    "        print (\"Building Word2Vec vocab...\")\n",
    "        model.build_vocab(cleaned_abstracts)\n",
    "        print (\"Loading intersect vectors...\")\n",
    "        model.intersect_word2vec_format(path_to_google_news + 'GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "        model.save(\"w2v_model\")\n",
    "        print (\"Model saved to disk\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) avg_number_citations_of_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to create a dictionary where we put the number of references from author1 to author2.\n",
    "\n",
    "Example if three references (three articles):\n",
    "\n",
    "    (author1, author2) : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one time\n",
    "def count_authLinksStoT (information_set, node_info):\n",
    "    authLinks = {}\n",
    "    for i in range(len(information_set)):\n",
    "        source = information_set[i][0]\n",
    "        target = information_set[i][1]\n",
    "\n",
    "        source_info = [element for element in node_info if element[0] == source][0]\n",
    "        target_info = [element for element in node_info if element[0] == target][0]\n",
    "\n",
    "        source_auth = source_info[3].split(\",\")\n",
    "        target_auth = target_info[3].split(\",\")\n",
    "        \n",
    "        for s in source_auth:\n",
    "            s.replace(' ', '')\n",
    "        for t in target_auth:\n",
    "            t.replace(' ', '')\n",
    "        \n",
    "        for s in source_auth:\n",
    "            for t in target_auth:\n",
    "                key = (s,t)\n",
    "                if key in authLinks:\n",
    "                    authLinks[key] += 1\n",
    "                else:\n",
    "                    authLinks[key] = 1\n",
    "    return authLink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the average number of citations the authors of target have received from authors of source FOR ONE RECORD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_number_citations(avg_number_citations_of_authors, source_auth, target_auth, authLinks):\n",
    "    summ = 0\n",
    "    count = 0\n",
    "    for s in source_auth:\n",
    "        for t in target_auth:\n",
    "            key = (s,t)\n",
    "            if key in authLinks:\n",
    "                summ += authLinks[key]\n",
    "                count += 1\n",
    "    if count == 0:\n",
    "        avg_number_citations_of_authors.append(0)\n",
    "    else:\n",
    "        avg_number_citations_of_authors.append(summ/count)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) shortest_path and edge_connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the full NON directed graph of relations between the abstracts to calculate the shortest path and the edge sonnectivity using igraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one time\n",
    "def undirected_graph(information_set, IDs):\n",
    "    edges = [(element[0],element[1]) for element in information_set if element[2]==\"1\"]\n",
    "    nodes = IDs\n",
    "\n",
    "    graph = igraph.Graph(directed=False)\n",
    "    graph.add_vertices(nodes)\n",
    "    graph.add_edges(edges)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to calculate a shortest edge FOR ONE RECORD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_edge_connectivity(shortest_path, edge_connectivity, edge, graph, source, target):\n",
    "    if int(edge) == 1:\n",
    "        graph.delete_edges([(source,target)])\n",
    "        val = graph.shortest_paths_dijkstra(source=index_source, target=index_target)[0][0]\n",
    "        edge_connectivity.append(graph.edge_disjoint_paths(source=index_source, target=index_target))\n",
    "        shortest_path.append(val)\n",
    "        graph.add_edges([(source,target)])\n",
    "    else:\n",
    "        val = graph.shortest_paths_dijkstra(source=index_source, target=index_target)[0][0]\n",
    "        edge_connectivity.append(graph.edge_disjoint_paths(source=index_source, target=index_target))\n",
    "        shortest_path.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean function from feture_engineering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s, stemmer, stpwds):\n",
    "    s = s.lower().split(\" \")\n",
    "    s = [token for token in s if token not in stpwds]\n",
    "    s = [stemmer.stem(token) for token in s]\n",
    "    s = [''.join([elt for elt in token if not elt.isdigit()]) for token in s] # remove digits\n",
    "    s = [token for token in s if len(token)>2] # remove tokens shorter than 3 characters in size\n",
    "    s = [token for token in s if len(token)<=25] # remove tokens exceeding 25 characters in size\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature_engineering function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering1step(information_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v):\n",
    "    \n",
    "    ### ---- Baseline features arrays ---- ####\n",
    "    # number of overlapping words in title\n",
    "    overlap_title = []\n",
    "    # temporal distance between the papers\n",
    "    temp_diff = []\n",
    "    # number of common authors\n",
    "    comm_auth = []\n",
    "    \n",
    "    ### ---- Generated features arrays ---- ####\n",
    "    # WMD\n",
    "    wmd = []\n",
    "    # number of references for the source or the target\n",
    "    num_references_source = []\n",
    "    num_references_target = []\n",
    "    # number of common neighbors\n",
    "    num_common_neighbors = []\n",
    "    # number of keywords: graph of words\n",
    "    # num_keywords_graph_of_words = []\n",
    "    # TF_IDF\n",
    "    pairwise_similarity_number = []\n",
    "    # the average number of citations the authors of target have received from authors of source\n",
    "    avg_number_citations_of_authors = []  \n",
    "    # shortest path between the nodes of abstract graph\n",
    "    shortest_path = []\n",
    "    \n",
    "    #### ---- Preparation functions ---- ####\n",
    "    # Authors link dictionary\n",
    "    authLinks = count_authLinksStoT(information_set, node_info)\n",
    "    # Undirected graph to calculate shortest_path and edge_connectivity\n",
    "    graph = undirected_graph(information_set, IDs)\n",
    "    # More useful variables\n",
    "    counter = 0\n",
    "    degrees = g.degree(IDs)\n",
    "    neighbors_list = []\n",
    "    for id in IDs:\n",
    "        neighbors_list.append(set(g.neighbors(id)))\n",
    "        \n",
    "    #### ---- Calculating features ---- ####\n",
    "    for i in range(len(information_set)):\n",
    "        source = information_set[i][0]\n",
    "        target = information_set[i][1]\n",
    "        edge = information_set[i][2]\n",
    "\n",
    "        index_source = IDs.index(source)\n",
    "        index_target = IDs.index(target)\n",
    "\n",
    "        source_info = [element for element in node_info if element[0] == source][0]\n",
    "        target_info = [element for element in node_info if element[0] == target][0]\n",
    "\n",
    "        source_title = clean(source_info[2], stemmer, stpwds)\n",
    "        target_title = clean(target_info[2], stemmer, stpwds)\n",
    "\n",
    "        source_auth = source_info[3].split(\",\")\n",
    "        target_auth = target_info[3].split(\",\")\n",
    "\n",
    "        source_abstract = clean(source_info[5], stemmer, stpwds)\n",
    "        target_abstract = clean(target_info[5], stemmer, stpwds)\n",
    "\n",
    "        # Baseline features\n",
    "        overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "        temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "        # Generated features\n",
    "        wmd.append(w2v.wv.wmdistance(source_abstract, target_abstract))\n",
    "        num_references_source.append(degrees[index_source])\n",
    "        num_references_target.append(degrees[index_target])\n",
    "        num_common_neighbors.append(len(neighbors_list[index_source].intersection(neighbors_list[index_target])))\n",
    "        avg_number_citations(avg_number_citations_of_authors, source_auth, target_auth, authLinks)\n",
    "        # num_keywords_graph_of_words.append(len(set(keywords_graph_of_words(source_abstract)).intersection(set(keywords_graph_of_words(target_abstract)))))\n",
    "        pairwise_similarity_number.append(pairwise_similarity[index_source, index_target])\n",
    "        shortest_path_edge_connectivity(shortest_path, edge_connectivity, edge, graph, source, target)\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 1000 == 0:\n",
    "            print (counter, \"examples processed\")\n",
    "            \n",
    "    print (\"All features calculated\")\n",
    "\n",
    "    #### ---- Final features array ---- ####\n",
    "    list_of_features = []\n",
    "    list_of_features.append(overlap_title)\n",
    "    list_of_features.append(temp_diff)\n",
    "    list_of_features.append(comm_auth)\n",
    "    list_of_features.append(wmd)\n",
    "    list_of_features.append(num_references_source)\n",
    "    list_of_features.append(num_references_target)\n",
    "    list_of_features.append(num_common_neighbors)\n",
    "    list_of_features.append(avg_number_citations_of_authors)\n",
    "    # list_of_features.append(num_keywords_graph_of_words)\n",
    "    list_of_features.append(pairwise_similarity_number)\n",
    "    list_of_features.append(shortest_path)\n",
    "    list_of_features.append(edge_connectivity)\n",
    "    \n",
    "    # Convert list of lists into array\n",
    "    # Documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "    features = np.array(list_of_features).T\n",
    "    # Scale\n",
    "    features = preprocessing.scale(features)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering2step(information_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v):\n",
    "    # shortest path between the nodes of abstract graph\n",
    "    shortest_path = []\n",
    "    edge_connectivity = []\n",
    "    \n",
    "    #### ---- Preparation functions ---- ####\n",
    "    # Authors link dictionary\n",
    "    authLinks = count_authLinksStoT(information_set, node_info)\n",
    "    # Undirected graph to calculate shortest_path and edge_connectivity\n",
    "    graph = undirected_graph(information_set, IDs)\n",
    "    # More useful variables\n",
    "    counter = 0\n",
    "    degrees = g.degree(IDs)\n",
    "    neighbors_list = []\n",
    "    for id in IDs:\n",
    "        neighbors_list.append(set(g.neighbors(id)))\n",
    "        \n",
    "    #### ---- Calculating features ---- ####\n",
    "    for i in range(len(information_set)):\n",
    "        source = information_set[i][0]\n",
    "        target = information_set[i][1]\n",
    "        edge = information_set[i][2]\n",
    "\n",
    "        index_source = IDs.index(source)\n",
    "        index_target = IDs.index(target)\n",
    "\n",
    "        source_info = [element for element in node_info if element[0] == source][0]\n",
    "        target_info = [element for element in node_info if element[0] == target][0]\n",
    "\n",
    "        source_title = clean(source_info[2], stemmer, stpwds)\n",
    "        target_title = clean(target_info[2], stemmer, stpwds)\n",
    "\n",
    "        source_auth = source_info[3].split(\",\")\n",
    "        target_auth = target_info[3].split(\",\")\n",
    "\n",
    "        source_abstract = clean(source_info[5], stemmer, stpwds)\n",
    "        target_abstract = clean(target_info[5], stemmer, stpwds)\n",
    "\n",
    "        shortest_path_edge_connectivity(shortest_path, edge_connectivity, edge, graph, source, target)\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 1000 == 0:\n",
    "            print (counter, \"examples processed\")\n",
    "            \n",
    "    print (\"All features calculated\")\n",
    "\n",
    "    #### ---- Final features array ---- ####\n",
    "    list_of_features = []\n",
    "    list_of_features.append(overlap_title)\n",
    "    list_of_features.append(temp_diff)\n",
    "    list_of_features.append(comm_auth)\n",
    "    list_of_features.append(wmd)\n",
    "    list_of_features.append(num_references_source)\n",
    "    list_of_features.append(num_references_target)\n",
    "    list_of_features.append(num_common_neighbors)\n",
    "    list_of_features.append(avg_number_citations_of_authors)\n",
    "    # list_of_features.append(num_keywords_graph_of_words)\n",
    "    list_of_features.append(pairwise_similarity_number)\n",
    "    list_of_features.append(shortest_path)\n",
    "    list_of_features.append(edge_connectivity)\n",
    "    \n",
    "    # Convert list of lists into array\n",
    "    # Documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "    features = np.array(list_of_features).T\n",
    "    # Scale\n",
    "    features = preprocessing.scale(features)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate w2v one time for training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model loaded\n"
     ]
    }
   ],
   "source": [
    "# W2V\n",
    "w2v = build_w2v(node_info, stemmer, stpwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate features and save it in files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = feature_engineering(training_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v)\n",
    "np.save(path_to_data + 'training_features_test_parted.npy', training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_features = feature_engineering(testing_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v)\n",
    "np.save(path_to_data + 'testing_features_test_parted.npy', testing_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
