{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering process Waldor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In Part 1 we will download data like we do in public_baseline. For this data we will calculate features after.\n",
    "In Part 2 we will start feature engineering - make some functions for calculating some generated features.\n",
    "In Part 3 we write modified feature_engineering function -> we add one new argumnet - w2v.\n",
    "In Part 4 we will do a feature_engineering to have all features we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk\n",
    "import csv\n",
    "import igraph\n",
    "import math\n",
    "\n",
    "from read_data import *\n",
    "from graph_creation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization like in public_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dinar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dinar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "creating graph\n",
      "graph created\n"
     ]
    }
   ],
   "source": [
    "# ---First Initializations--- #\n",
    "path_to_data = \"../data/\"\n",
    "nltk.download('punkt')  # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# ---Read Data--- #\n",
    "testing_set, training_set, node_info = read_data()\n",
    "IDs = [element[0] for element in node_info]\n",
    "\n",
    "# ---Compute TFIDF vector of each paper--- #\n",
    "corpus = [element[5] for element in node_info]\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "# each row is a node in the order of node_info\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)\n",
    "pairwise_similarity = features_TFIDF * features_TFIDF.T\n",
    "#print pairwise_similarity.shape\n",
    "# ---Create graph--- #\n",
    "g = create_graph(training_set, IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional to public_baseline graph features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_w2v(node_info, stemmer, stpwds):\n",
    "    try:\n",
    "        model = Word2Vec.load(\"w2v_model\")\n",
    "        print (\"Word2Vec model loaded\")\n",
    "    except:\n",
    "        path_to_google_news = '../data'\n",
    "        my_q = 300 # to match dim of GNews word vectors\n",
    "        mcount = 5\n",
    "        model = Word2Vec(size=my_q, min_count=mcount)\n",
    "        cleaned_abstracts = [clean(element[5], stemmer, stpwds) for element in node_info]\n",
    "        print (\"Building Word2Vec vocab...\")\n",
    "        model.build_vocab(cleaned_abstracts)\n",
    "        print (\"Loading intersect vectors...\")\n",
    "        model.intersect_word2vec_format(path_to_google_news + 'GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "        model.save(\"w2v_model\")\n",
    "        print (\"Model saved to disk\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) avg_number_citations_of_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to create a dictionary where we put the number of references from author1 to author2.\n",
    "\n",
    "Example if three references (three articles):\n",
    "\n",
    "    (author1, author2) : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one time\n",
    "def count_authLinksStoT (information_set, node_info):\n",
    "    authLinks = {}\n",
    "    counter = 0\n",
    "    for i in range(len(information_set)):\n",
    "        source = information_set[i][0]\n",
    "        target = information_set[i][1]\n",
    "\n",
    "        source_info = [element for element in node_info if element[0] == source][0]\n",
    "        target_info = [element for element in node_info if element[0] == target][0]\n",
    "\n",
    "        source_auth = source_info[3].split(\",\")\n",
    "        target_auth = target_info[3].split(\",\")\n",
    "        \n",
    "        for s in source_auth:\n",
    "            s.replace(' ', '')\n",
    "        for t in target_auth:\n",
    "            t.replace(' ', '')\n",
    "        \n",
    "        for s in source_auth:\n",
    "            for t in target_auth:\n",
    "                key = (s,t)\n",
    "                if key in authLinks:\n",
    "                    authLinks[key] += 1\n",
    "                else:\n",
    "                    authLinks[key] = 1\n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "            print (counter, \"examples processed\")\n",
    "    return authLinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the average number of citations the authors of target have received from authors of source FOR ONE RECORD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_number_citations(avg_number_citations_of_authors, source_auth, target_auth, authLinks):\n",
    "    summ = 0\n",
    "    count = 0\n",
    "    for s in source_auth:\n",
    "        for t in target_auth:\n",
    "            key = (s,t)\n",
    "            if key in authLinks:\n",
    "                summ += authLinks[key]\n",
    "                count += 1\n",
    "    if count == 0:\n",
    "        avg_number_citations_of_authors.append(0)\n",
    "    else:\n",
    "        avg_number_citations_of_authors.append(summ/count)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) shortest_path and edge_connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the full NON directed graph of relations between the abstracts to calculate the shortest path and the edge sonnectivity using igraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one time\n",
    "def undirected_graph(information_set, IDs):\n",
    "    edges = [(element[0], element[1]) for element in information_set if element[2]==\"1\"]\n",
    "    nodes = IDs\n",
    "\n",
    "    print (\"Edges and nodes for undirected graph prepared.\")\n",
    "    graph = igraph.Graph(directed=False)\n",
    "    graph.add_vertices(nodes)\n",
    "    graph.add_edges(edges)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to calculate a shortest edge FOR ONE RECORD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_edge_connectivity(shortest_path, edge_connectivity, edge, graph, source, target):\n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    if (edge == \"1\"):\n",
    "        graph.delete_edges([(source,target)])\n",
    "        val = graph.shortest_paths_dijkstra(source=index_source, target=index_target)[0][0]\n",
    "        edge_connectivity.append(graph.edge_disjoint_paths(source=index_source, target=index_target))\n",
    "        shortest_path.append(val)\n",
    "        graph.add_edges([(source,target)])\n",
    "    else:\n",
    "        val = graph.shortest_paths_dijkstra(source=index_source, target=index_target)[0][0]\n",
    "        edge_connectivity.append(graph.edge_disjoint_paths(source=index_source, target=index_target))\n",
    "        shortest_path.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean function from feture_engineering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s, stemmer, stpwds):\n",
    "    s = s.lower().split(\" \")\n",
    "    s = [token for token in s if token not in stpwds]\n",
    "    s = [stemmer.stem(token) for token in s]\n",
    "    s = [''.join([elt for elt in token if not elt.isdigit()]) for token in s] # remove digits\n",
    "    s = [token for token in s if len(token)>2] # remove tokens shorter than 3 characters in size\n",
    "    s = [token for token in s if len(token)<=25] # remove tokens exceeding 25 characters in size\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature_engineering function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_step (information_set, node_info, IDs) :\n",
    "    # Undirected graph to calculate shortest_path and edge_connectivity\n",
    "    graph = undirected_graph(information_set, IDs)\n",
    "    print (\"Undirected graph created.\")\n",
    "    print (\"First step finished.\\n\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_step (information_set, IDs, node_info, stemmer, stpwds, g): \n",
    "    # the average number of citations the authors of target have received from authors of source\n",
    "    avg_number_citations_of_authors = [] \n",
    "    \n",
    "    # More useful variables\n",
    "    counter = 0\n",
    "    degrees = g.degree(IDs)\n",
    "    neighbors_list = []\n",
    "    for id in IDs:\n",
    "        neighbors_list.append(set(g.neighbors(id)))\n",
    "        \n",
    "    print (\"Second step - start to calculate features.\")\n",
    "    \n",
    "    #### ---- Calculating features ---- ####\n",
    "    for i in range(len(information_set)):\n",
    "        source = information_set[i][0]\n",
    "        target = information_set[i][1]\n",
    "\n",
    "        source_info = [element for element in node_info if element[0] == source][0]\n",
    "        target_info = [element for element in node_info if element[0] == target][0]\n",
    "\n",
    "        source_auth = source_info[3].split(\",\")\n",
    "        target_auth = target_info[3].split(\",\")\n",
    "\n",
    "        # Generated features\n",
    "        avg_number_citations(avg_number_citations_of_authors, source_auth, target_auth, authLinks)\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "            print (counter, \"examples processed\")\n",
    "            \n",
    "    print (\"2 step - All features calculated\")\n",
    "\n",
    "    #### ---- Final features array ---- ####\n",
    "    list_of_features = []\n",
    "    list_of_features.append(avg_number_citations_of_authors)\n",
    "    \n",
    "    return list_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_step (information_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v):    \n",
    "    ### ---- Baseline features arrays ---- ####\n",
    "    # number of overlapping words in title\n",
    "    overlap_title = []\n",
    "    # temporal distance between the papers\n",
    "    temp_diff = []\n",
    "    # number of common authors\n",
    "    comm_auth = []\n",
    "    \n",
    "    ### ---- Generated features arrays ---- ####\n",
    "    # WMD\n",
    "    wmd = []\n",
    "    # number of references for the source or the target\n",
    "    num_references_source = []\n",
    "    num_references_target = []\n",
    "    # number of common neighbors\n",
    "    num_common_neighbors = []\n",
    "    # number of keywords: graph of words\n",
    "    # num_keywords_graph_of_words = []\n",
    "    # TF_IDF\n",
    "    pairwise_similarity_number = []\n",
    "    \n",
    "    # More useful variables\n",
    "    counter = 0\n",
    "    degrees = g.degree(IDs)\n",
    "    neighbors_list = []\n",
    "    for id in IDs:\n",
    "        neighbors_list.append(set(g.neighbors(id)))\n",
    "        \n",
    "    print (\"Third step - start to calculate features.\")\n",
    "    \n",
    "    #### ---- Calculating features ---- ####\n",
    "    for i in range(len(information_set)):\n",
    "        source = information_set[i][0]\n",
    "        target = information_set[i][1]\n",
    "        edge = information_set[i][2]\n",
    "\n",
    "        index_source = IDs.index(source)\n",
    "        index_target = IDs.index(target)\n",
    "\n",
    "        source_info = [element for element in node_info if element[0] == source][0]\n",
    "        target_info = [element for element in node_info if element[0] == target][0]\n",
    "\n",
    "        source_title = clean(source_info[2], stemmer, stpwds)\n",
    "        target_title = clean(target_info[2], stemmer, stpwds)\n",
    "\n",
    "        source_auth = source_info[3].split(\",\")\n",
    "        target_auth = target_info[3].split(\",\")\n",
    "\n",
    "        source_abstract = clean(source_info[5], stemmer, stpwds)\n",
    "        target_abstract = clean(target_info[5], stemmer, stpwds)\n",
    "\n",
    "        # Baseline features\n",
    "        overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "        temp_diff.append(int(source_info[1]) - int(target_info[1]))\n",
    "        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "        # Generated features\n",
    "        wmd.append(w2v.wv.wmdistance(source_abstract, target_abstract))\n",
    "        num_references_source.append(degrees[index_source])\n",
    "        num_references_target.append(degrees[index_target])\n",
    "        num_common_neighbors.append(len(neighbors_list[index_source].intersection(neighbors_list[index_target])))\n",
    "        # num_keywords_graph_of_words.append(len(set(keywords_graph_of_words(source_abstract)).intersection(set(keywords_graph_of_words(target_abstract)))))\n",
    "        pairwise_similarity_number.append(pairwise_similarity[index_source, index_target])\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 1000 == 0:\n",
    "            print (counter, \"examples processed\")\n",
    "            \n",
    "    print (\"3 step - All features calculated\")\n",
    "\n",
    "    #### ---- Final features array ---- ####\n",
    "    list_of_features = []\n",
    "    list_of_features.append(overlap_title)\n",
    "    list_of_features.append(temp_diff)\n",
    "    list_of_features.append(comm_auth)\n",
    "    list_of_features.append(wmd)\n",
    "    list_of_features.append(num_references_source)\n",
    "    list_of_features.append(num_references_target)\n",
    "    list_of_features.append(num_common_neighbors)\n",
    "    # list_of_features.append(num_keywords_graph_of_words)\n",
    "    list_of_features.append(pairwise_similarity_number)\n",
    "    \n",
    "    return list_of_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forth_step(information_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v, graph, TEST):\n",
    "    # shortest path between the nodes of abstract graph\n",
    "    shortest_path = []\n",
    "    edge_connectivity = []\n",
    "    \n",
    "    # More useful variables\n",
    "    counter = 0\n",
    "    degrees = g.degree(IDs)\n",
    "    neighbors_list = []\n",
    "    for id in IDs:\n",
    "        neighbors_list.append(set(g.neighbors(id)))\n",
    "        \n",
    "    print (\"Forth step - start to calculate features.\")\n",
    "        \n",
    "    #### ---- Calculating features ---- ####\n",
    "    for i in range(len(information_set)):\n",
    "        source = information_set[i][0]\n",
    "        target = information_set[i][1]\n",
    "        if TEST :\n",
    "            edge = 0\n",
    "        else:\n",
    "            edge = information_set[i][2]\n",
    "\n",
    "        shortest_path_edge_connectivity(shortest_path, edge_connectivity, edge, graph, source, target)\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 100 == 0:\n",
    "            print (counter, \"examples processed\")\n",
    "    \n",
    "    print (\"4 step - All features calculated\")\n",
    "\n",
    "    #### ---- Final features array ---- ####\n",
    "    list_of_features.append(shortest_path)\n",
    "    list_of_features.append(edge_connectivity)\n",
    "        \n",
    "    return list_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate w2v one time for training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model loaded\n"
     ]
    }
   ],
   "source": [
    "# W2V\n",
    "w2v = build_w2v(node_info, stemmer, stpwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate features and save it in files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commun part for trainig and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of set :  615512\n",
      "10000 examples processed\n",
      "20000 examples processed\n",
      "30000 examples processed\n",
      "40000 examples processed\n",
      "50000 examples processed\n",
      "60000 examples processed\n",
      "70000 examples processed\n",
      "80000 examples processed\n",
      "90000 examples processed\n",
      "100000 examples processed\n",
      "110000 examples processed\n",
      "120000 examples processed\n",
      "130000 examples processed\n",
      "140000 examples processed\n",
      "150000 examples processed\n",
      "160000 examples processed\n",
      "170000 examples processed\n",
      "180000 examples processed\n",
      "190000 examples processed\n",
      "200000 examples processed\n",
      "210000 examples processed\n",
      "220000 examples processed\n",
      "230000 examples processed\n",
      "240000 examples processed\n",
      "250000 examples processed\n",
      "260000 examples processed\n",
      "270000 examples processed\n",
      "280000 examples processed\n",
      "290000 examples processed\n",
      "300000 examples processed\n",
      "310000 examples processed\n",
      "320000 examples processed\n",
      "330000 examples processed\n",
      "340000 examples processed\n",
      "350000 examples processed\n",
      "360000 examples processed\n",
      "370000 examples processed\n",
      "380000 examples processed\n",
      "390000 examples processed\n",
      "400000 examples processed\n",
      "410000 examples processed\n",
      "420000 examples processed\n",
      "430000 examples processed\n",
      "440000 examples processed\n",
      "450000 examples processed\n",
      "460000 examples processed\n",
      "470000 examples processed\n",
      "480000 examples processed\n",
      "490000 examples processed\n",
      "500000 examples processed\n",
      "510000 examples processed\n",
      "520000 examples processed\n",
      "530000 examples processed\n",
      "540000 examples processed\n",
      "550000 examples processed\n",
      "560000 examples processed\n",
      "570000 examples processed\n",
      "580000 examples processed\n",
      "590000 examples processed\n",
      "600000 examples processed\n",
      "610000 examples processed\n",
      "Dictonary authLinks created.\n"
     ]
    }
   ],
   "source": [
    "#### ---- Preparation functions ---- ####\n",
    "\n",
    "## First step ##\n",
    "print (\"Length of set : \", len(training_set))\n",
    "authLinks = {}\n",
    "# Authors link dictionary\n",
    "authLinks = count_authLinksStoT(training_set, node_info)\n",
    "print (\"Dictonary authLinks created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges and nodes for undirected graph prepared.\n",
      "Undirected graph created.\n",
      "First step finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph = first_step (training_set, node_info, IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features calculating.\n"
     ]
    }
   ],
   "source": [
    "print(\"Train features calculating.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second step - start to calculate features.\n",
      "10000 examples processed\n",
      "20000 examples processed\n",
      "30000 examples processed\n",
      "40000 examples processed\n",
      "50000 examples processed\n",
      "60000 examples processed\n",
      "70000 examples processed\n",
      "80000 examples processed\n",
      "90000 examples processed\n",
      "100000 examples processed\n",
      "110000 examples processed\n",
      "120000 examples processed\n",
      "130000 examples processed\n",
      "140000 examples processed\n",
      "150000 examples processed\n",
      "160000 examples processed\n",
      "170000 examples processed\n",
      "180000 examples processed\n",
      "190000 examples processed\n",
      "200000 examples processed\n",
      "210000 examples processed\n",
      "220000 examples processed\n",
      "230000 examples processed\n",
      "240000 examples processed\n",
      "250000 examples processed\n",
      "260000 examples processed\n",
      "270000 examples processed\n",
      "280000 examples processed\n",
      "290000 examples processed\n",
      "300000 examples processed\n",
      "310000 examples processed\n",
      "320000 examples processed\n",
      "330000 examples processed\n",
      "340000 examples processed\n",
      "350000 examples processed\n",
      "360000 examples processed\n",
      "370000 examples processed\n",
      "380000 examples processed\n",
      "390000 examples processed\n",
      "400000 examples processed\n",
      "410000 examples processed\n",
      "420000 examples processed\n",
      "430000 examples processed\n",
      "440000 examples processed\n",
      "450000 examples processed\n",
      "460000 examples processed\n",
      "470000 examples processed\n",
      "480000 examples processed\n",
      "490000 examples processed\n",
      "500000 examples processed\n",
      "510000 examples processed\n",
      "520000 examples processed\n",
      "530000 examples processed\n",
      "540000 examples processed\n",
      "550000 examples processed\n",
      "560000 examples processed\n",
      "570000 examples processed\n",
      "580000 examples processed\n",
      "590000 examples processed\n",
      "600000 examples processed\n",
      "610000 examples processed\n",
      "2 step - All features calculated\n",
      "Average links calculated.\n"
     ]
    }
   ],
   "source": [
    "## Second step ##\n",
    "avg_auth_train = second_step (training_set, IDs, node_info, stemmer, stpwds, g)\n",
    "print(\"Average links calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_auth_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ebdaef8435b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfeatures_train\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeatures_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_auth_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_auth_train' is not defined"
     ]
    }
   ],
   "source": [
    "features_train= []\n",
    "#features_train.append(avg_auth_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third step - start to calculate features.\n",
      "1000 examples processed\n",
      "2000 examples processed\n",
      "3000 examples processed\n",
      "4000 examples processed\n",
      "5000 examples processed\n",
      "6000 examples processed\n",
      "7000 examples processed\n",
      "8000 examples processed\n",
      "9000 examples processed\n",
      "10000 examples processed\n",
      "11000 examples processed\n",
      "12000 examples processed\n",
      "13000 examples processed\n",
      "14000 examples processed\n",
      "15000 examples processed\n",
      "16000 examples processed\n",
      "17000 examples processed\n",
      "18000 examples processed\n",
      "19000 examples processed\n",
      "20000 examples processed\n",
      "21000 examples processed\n",
      "22000 examples processed\n",
      "23000 examples processed\n",
      "24000 examples processed\n",
      "25000 examples processed\n",
      "26000 examples processed\n",
      "27000 examples processed\n",
      "28000 examples processed\n",
      "29000 examples processed\n",
      "30000 examples processed\n",
      "31000 examples processed\n",
      "32000 examples processed\n",
      "33000 examples processed\n",
      "34000 examples processed\n",
      "35000 examples processed\n",
      "36000 examples processed\n",
      "37000 examples processed\n",
      "38000 examples processed\n",
      "39000 examples processed\n",
      "40000 examples processed\n",
      "41000 examples processed\n",
      "42000 examples processed\n",
      "43000 examples processed\n",
      "44000 examples processed\n",
      "45000 examples processed\n",
      "46000 examples processed\n",
      "47000 examples processed\n",
      "48000 examples processed\n",
      "49000 examples processed\n",
      "50000 examples processed\n",
      "51000 examples processed\n",
      "52000 examples processed\n",
      "53000 examples processed\n",
      "54000 examples processed\n",
      "55000 examples processed\n",
      "56000 examples processed\n",
      "57000 examples processed\n",
      "58000 examples processed\n",
      "59000 examples processed\n",
      "60000 examples processed\n",
      "61000 examples processed\n",
      "62000 examples processed\n",
      "63000 examples processed\n",
      "64000 examples processed\n",
      "65000 examples processed\n",
      "66000 examples processed\n",
      "67000 examples processed\n",
      "68000 examples processed\n",
      "69000 examples processed\n",
      "70000 examples processed\n",
      "71000 examples processed\n",
      "72000 examples processed\n",
      "73000 examples processed\n",
      "74000 examples processed\n",
      "75000 examples processed\n",
      "76000 examples processed\n",
      "77000 examples processed\n",
      "78000 examples processed\n",
      "79000 examples processed\n",
      "80000 examples processed\n",
      "81000 examples processed\n",
      "82000 examples processed\n",
      "83000 examples processed\n",
      "84000 examples processed\n",
      "85000 examples processed\n",
      "86000 examples processed\n",
      "87000 examples processed\n",
      "88000 examples processed\n",
      "89000 examples processed\n",
      "90000 examples processed\n",
      "91000 examples processed\n",
      "92000 examples processed\n",
      "93000 examples processed\n",
      "94000 examples processed\n",
      "95000 examples processed\n",
      "96000 examples processed\n",
      "97000 examples processed\n",
      "98000 examples processed\n",
      "99000 examples processed\n",
      "100000 examples processed\n",
      "101000 examples processed\n",
      "102000 examples processed\n",
      "103000 examples processed\n",
      "104000 examples processed\n",
      "105000 examples processed\n",
      "106000 examples processed\n",
      "107000 examples processed\n",
      "108000 examples processed\n",
      "109000 examples processed\n",
      "110000 examples processed\n",
      "111000 examples processed\n",
      "112000 examples processed\n",
      "113000 examples processed\n",
      "114000 examples processed\n",
      "115000 examples processed\n",
      "116000 examples processed\n",
      "117000 examples processed\n",
      "118000 examples processed\n",
      "119000 examples processed\n",
      "120000 examples processed\n",
      "121000 examples processed\n",
      "122000 examples processed\n",
      "123000 examples processed\n",
      "124000 examples processed\n",
      "125000 examples processed\n",
      "126000 examples processed\n",
      "127000 examples processed\n",
      "128000 examples processed\n",
      "129000 examples processed\n",
      "130000 examples processed\n",
      "131000 examples processed\n",
      "132000 examples processed\n",
      "133000 examples processed\n",
      "134000 examples processed\n",
      "135000 examples processed\n",
      "136000 examples processed\n",
      "137000 examples processed\n",
      "138000 examples processed\n",
      "139000 examples processed\n",
      "140000 examples processed\n",
      "141000 examples processed\n",
      "142000 examples processed\n",
      "143000 examples processed\n",
      "144000 examples processed\n",
      "145000 examples processed\n",
      "146000 examples processed\n",
      "147000 examples processed\n",
      "148000 examples processed\n",
      "149000 examples processed\n",
      "150000 examples processed\n",
      "151000 examples processed\n",
      "152000 examples processed\n",
      "153000 examples processed\n",
      "154000 examples processed\n",
      "155000 examples processed\n",
      "156000 examples processed\n",
      "157000 examples processed\n",
      "158000 examples processed\n",
      "159000 examples processed\n",
      "160000 examples processed\n",
      "161000 examples processed\n",
      "162000 examples processed\n",
      "163000 examples processed\n",
      "164000 examples processed\n",
      "165000 examples processed\n",
      "166000 examples processed\n",
      "167000 examples processed\n",
      "168000 examples processed\n",
      "169000 examples processed\n",
      "170000 examples processed\n",
      "171000 examples processed\n",
      "172000 examples processed\n",
      "173000 examples processed\n",
      "174000 examples processed\n",
      "175000 examples processed\n",
      "176000 examples processed\n",
      "177000 examples processed\n",
      "178000 examples processed\n",
      "179000 examples processed\n",
      "180000 examples processed\n",
      "181000 examples processed\n",
      "182000 examples processed\n",
      "183000 examples processed\n",
      "184000 examples processed\n",
      "185000 examples processed\n",
      "186000 examples processed\n",
      "187000 examples processed\n",
      "188000 examples processed\n",
      "189000 examples processed\n",
      "190000 examples processed\n",
      "191000 examples processed\n",
      "192000 examples processed\n",
      "193000 examples processed\n",
      "194000 examples processed\n",
      "195000 examples processed\n",
      "196000 examples processed\n",
      "197000 examples processed\n",
      "198000 examples processed\n",
      "199000 examples processed\n",
      "200000 examples processed\n",
      "201000 examples processed\n",
      "202000 examples processed\n",
      "203000 examples processed\n",
      "204000 examples processed\n",
      "205000 examples processed\n",
      "206000 examples processed\n",
      "207000 examples processed\n",
      "208000 examples processed\n",
      "209000 examples processed\n",
      "210000 examples processed\n",
      "211000 examples processed\n",
      "212000 examples processed\n",
      "213000 examples processed\n",
      "214000 examples processed\n",
      "215000 examples processed\n",
      "216000 examples processed\n",
      "217000 examples processed\n",
      "218000 examples processed\n",
      "219000 examples processed\n",
      "220000 examples processed\n",
      "221000 examples processed\n",
      "222000 examples processed\n",
      "223000 examples processed\n",
      "224000 examples processed\n",
      "225000 examples processed\n",
      "226000 examples processed\n",
      "227000 examples processed\n",
      "228000 examples processed\n",
      "229000 examples processed\n",
      "230000 examples processed\n",
      "231000 examples processed\n",
      "232000 examples processed\n",
      "233000 examples processed\n",
      "234000 examples processed\n",
      "235000 examples processed\n",
      "236000 examples processed\n",
      "237000 examples processed\n",
      "238000 examples processed\n",
      "239000 examples processed\n",
      "240000 examples processed\n",
      "241000 examples processed\n",
      "242000 examples processed\n",
      "243000 examples processed\n",
      "244000 examples processed\n",
      "245000 examples processed\n",
      "246000 examples processed\n",
      "247000 examples processed\n",
      "248000 examples processed\n",
      "249000 examples processed\n",
      "250000 examples processed\n",
      "251000 examples processed\n",
      "252000 examples processed\n",
      "253000 examples processed\n",
      "254000 examples processed\n",
      "255000 examples processed\n"
     ]
    }
   ],
   "source": [
    "## Third step ##\n",
    "features_train = third_step (training_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forth step - start to calculate features.\n",
      "100 examples processed\n",
      "200 examples processed\n",
      "300 examples processed\n",
      "400 examples processed\n",
      "500 examples processed\n",
      "600 examples processed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "no edge from vertex #21937 to #17814",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-6873464265a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Forth step ##\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeatures_4step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforth_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIDs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstpwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpairwise_similarity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-4ac6d873ca35>\u001b[0m in \u001b[0;36mforth_step\u001b[1;34m(information_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v, graph)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0medge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minformation_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mshortest_path_edge_connectivity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshortest_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_connectivity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-de53d8951bf8>\u001b[0m in \u001b[0;36mshortest_path_edge_connectivity\u001b[1;34m(shortest_path, edge_connectivity, edge, graph, source, target)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mindex_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIDs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0medge\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshortest_paths_dijkstra\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0medge_connectivity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medge_disjoint_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_source\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\igraph\\__init__.py\u001b[0m in \u001b[0;36mdelete_edges\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m             \u001b[0medge_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mGraphBase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: no edge from vertex #21937 to #17814"
     ]
    }
   ],
   "source": [
    "## Forth step ##\n",
    "features_4step = forth_step(training_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v, graph, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put all features together ##\n",
    "print(\"Putting all features together...\")\n",
    "features_train.append(features_4step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of lists into array\n",
    "# Documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "training_features = np.array(features_train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "#training_features = preprocessing.scale(training_features)\n",
    "np.save(path_to_data + 'avg_auth_train.npy', training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n Test features calculating.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second step - start to calculate features.\n",
      "10000 examples processed\n",
      "20000 examples processed\n",
      "30000 examples processed\n",
      "2 step - All features calculated\n",
      "Average links for test calculated.\n"
     ]
    }
   ],
   "source": [
    "## Second step ##\n",
    "avg_auth_test = second_step (testing_set, IDs, node_info, stemmer, stpwds, g)\n",
    "print(\"Average links for test calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test= []\n",
    "features_test.append(avg_auth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Third step ##\n",
    "features_test = third_step (testing_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forth step ##\n",
    "features_4step_test = forth_step(testing_set, IDs, node_info, stemmer, stpwds, g, pairwise_similarity, w2v, graph, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put all features together ##\n",
    "print(\"Putting all features together...\")\n",
    "features_test.append(features_4step_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of lists into array\n",
    "# Documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "testing_features = np.array(features_test).T\n",
    "# Scale\n",
    "#testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path_to_data + 'avg_auth_test.npy', testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
