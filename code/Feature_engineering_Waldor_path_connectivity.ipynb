{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering process Waldor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In Part 1 we will download data like we do in public_baseline. For this data we will calculate features after.\n",
    "In Part 2 we will start feature engineering - make some functions for calculating some generated features.\n",
    "In Part 3 we write modified feature_engineering function -> we add one new argumnet - w2v.\n",
    "In Part 4 we will do a feature_engineering to have all features we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries that we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dinar\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk\n",
    "import csv\n",
    "import igraph\n",
    "import math\n",
    "\n",
    "from read_data import *\n",
    "from graph_creation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization like in public_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dinar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dinar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "creating graph\n",
      "graph created\n"
     ]
    }
   ],
   "source": [
    "# ---First Initializations--- #\n",
    "path_to_data = \"../data/\"\n",
    "nltk.download('punkt')  # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# ---Read Data--- #\n",
    "testing_set, training_set, node_info = read_data()\n",
    "IDs = [element[0] for element in node_info]\n",
    "\n",
    "# ---Create graph--- #\n",
    "g = create_graph(training_set, IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional to public_baseline graph features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) shortest_path and edge_connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the full NON directed graph of relations between the abstracts to calculate the shortest path and the edge sonnectivity using igraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one time\n",
    "def undirected_graph(information_set, IDs):\n",
    "    edges = [(element[0], element[1]) for element in information_set if element[2]==\"1\"]\n",
    "    print(edges[0], edges[100], edges[100000])\n",
    "    nodes = IDs\n",
    "\n",
    "    print (\"Edges and nodes for undirected graph prepared.\")\n",
    "    graph = igraph.Graph(directed=False)\n",
    "    graph.add_vertices(nodes)\n",
    "    graph.add_edges(edges)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to calculate a shortest edge FOR ONE RECORD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_edge_connectivity(shortest_path, edge_connectivity, edge, graph, source, target):\n",
    "    index_source = IDs.index(source)\n",
    "    index_target = IDs.index(target)\n",
    "    if (edge == \"1\"):\n",
    "        graph.delete_edges([(source,target)])\n",
    "        val = graph.shortest_paths_dijkstra(source=index_source, target=index_target)[0][0]\n",
    "        edge_connectivity.append(graph.edge_disjoint_paths(source=index_source, target=index_target))\n",
    "        shortest_path.append(val)\n",
    "        graph.add_edges([(source,target)])\n",
    "    else:\n",
    "        val = graph.shortest_paths_dijkstra(source=index_source, target=index_target)[0][0]\n",
    "        edge_connectivity.append(graph.edge_disjoint_paths(source=index_source, target=index_target))\n",
    "        shortest_path.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean function from feture_engineering.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(s, stemmer, stpwds):\n",
    "    s = s.lower().split(\" \")\n",
    "    s = [token for token in s if token not in stpwds]\n",
    "    s = [stemmer.stem(token) for token in s]\n",
    "    s = [''.join([elt for elt in token if not elt.isdigit()]) for token in s] # remove digits\n",
    "    s = [token for token in s if len(token)>2] # remove tokens shorter than 3 characters in size\n",
    "    s = [token for token in s if len(token)<=25] # remove tokens exceeding 25 characters in size\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature_engineering function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def first_step (information_set, node_info, IDs) :\n",
    "    # Undirected graph to calculate shortest_path and edge_connectivity\n",
    " #   graph = undirected_graph(information_set, IDs)\n",
    "  #  print (\"Undirected graph created.\")\n",
    "   # print (\"First step finished.\\n\")\n",
    "    #return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('9510123', '9502114') ('205265', '105095') ('101119', '1082')\n",
      "Edges and nodes for undirected graph prepared.\n",
      "27770 335130\n"
     ]
    }
   ],
   "source": [
    "edges = [(element[0], element[1]) for element in training_set if element[2]==\"1\"]\n",
    "print(edges[0], edges[100], edges[100000])\n",
    "nodes = IDs\n",
    "\n",
    "print (\"Edges and nodes for undirected graph prepared.\")\n",
    "graph = igraph.Graph(directed=False)\n",
    "graph.add_vertices(nodes)\n",
    "graph.add_edges(edges)\n",
    "\n",
    "print(len(graph.vs), len(graph.es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335130"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forth_step(information_set, IDs, node_info, stemmer, stpwds, g, TEST):\n",
    "    # shortest path between the nodes of abstract graph\n",
    "    shortest_path = []\n",
    "    edge_connectivity = []\n",
    "    \n",
    "    # More useful variables\n",
    "    counter = 0\n",
    "    degrees = g.degree(IDs)\n",
    "    neighbors_list = []\n",
    "    for id in IDs:\n",
    "        neighbors_list.append(set(g.neighbors(id)))\n",
    "        \n",
    "    print (\"Forth step - start to calculate features.\")\n",
    "        \n",
    "    #### ---- Calculating features ---- ####\n",
    "    for i in range(len(information_set)):\n",
    "        source = information_set[i][0]\n",
    "        target = information_set[i][1]\n",
    "        if TEST :\n",
    "            edge = 0\n",
    "        else:\n",
    "            edge = information_set[i][2]\n",
    "            \n",
    "        index_source = IDs.index(source)\n",
    "        index_target = IDs.index(target)\n",
    "        \n",
    "     #   if source == '105155' and target == '9806044':\n",
    "      #      counter += 1\n",
    "       #     continue\n",
    "            \n",
    "        if (edge == \"1\"):\n",
    "            graph.delete_edges([(index_source, index_target)])\n",
    "            val = graph.shortest_paths_dijkstra(source=index_source, target=index_target)[0][0]\n",
    "            edge_connectivity.append(graph.edge_disjoint_paths(source=index_source, target=index_target))\n",
    "            shortest_path.append(val)\n",
    "            graph.add_edges([(source,target)])\n",
    "        else:\n",
    "            val = graph.shortest_paths_dijkstra(source=index_source, target=index_target)[0][0]\n",
    "            edge_connectivity.append(graph.edge_disjoint_paths(source=index_source, target=index_target))\n",
    "            shortest_path.append(val)\n",
    "\n",
    "      ##  shortest_path_edge_connectivity(shortest_path, edge_connectivity, edge, graph, source, target)\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            print (\"\\t\\t\\t\", counter, \"examples processed\")\n",
    "    \n",
    "    print (\"4 step - All features calculated\")\n",
    "\n",
    "    #### ---- Final features array ---- ####\n",
    "    list_of_features.append(shortest_path)\n",
    "    list_of_features.append(edge_connectivity)\n",
    "        \n",
    "    return list_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate features and save it in files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commun part for trainig and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('9510123', '9502114') ('205265', '105095') ('101119', '1082')\n",
      "Edges and nodes for undirected graph prepared.\n",
      "Undirected graph created.\n",
      "First step finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#graph = first_step (training_set, node_info, IDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features calculating.\n"
     ]
    }
   ],
   "source": [
    "print(\"Train features calculating.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forth step - start to calculate features.\n",
      "\t\t\t 10 examples processed\n",
      "\t\t\t 20 examples processed\n",
      "\t\t\t 30 examples processed\n",
      "\t\t\t 40 examples processed\n",
      "\t\t\t 50 examples processed\n",
      "\t\t\t 60 examples processed\n",
      "\t\t\t 70 examples processed\n",
      "\t\t\t 80 examples processed\n",
      "\t\t\t 90 examples processed\n",
      "\t\t\t 100 examples processed\n",
      "\t\t\t 110 examples processed\n",
      "\t\t\t 120 examples processed\n",
      "\t\t\t 130 examples processed\n",
      "\t\t\t 140 examples processed\n",
      "\t\t\t 150 examples processed\n",
      "\t\t\t 160 examples processed\n",
      "\t\t\t 170 examples processed\n",
      "\t\t\t 180 examples processed\n",
      "\t\t\t 190 examples processed\n",
      "\t\t\t 200 examples processed\n",
      "\t\t\t 210 examples processed\n",
      "\t\t\t 220 examples processed\n",
      "\t\t\t 230 examples processed\n",
      "\t\t\t 240 examples processed\n",
      "\t\t\t 250 examples processed\n",
      "\t\t\t 260 examples processed\n",
      "\t\t\t 270 examples processed\n",
      "\t\t\t 280 examples processed\n",
      "\t\t\t 290 examples processed\n",
      "\t\t\t 300 examples processed\n",
      "\t\t\t 310 examples processed\n",
      "\t\t\t 320 examples processed\n",
      "\t\t\t 330 examples processed\n",
      "\t\t\t 340 examples processed\n",
      "\t\t\t 350 examples processed\n",
      "\t\t\t 360 examples processed\n",
      "\t\t\t 370 examples processed\n",
      "\t\t\t 380 examples processed\n",
      "\t\t\t 390 examples processed\n",
      "\t\t\t 400 examples processed\n",
      "\t\t\t 410 examples processed\n",
      "\t\t\t 420 examples processed\n",
      "\t\t\t 430 examples processed\n",
      "\t\t\t 440 examples processed\n",
      "\t\t\t 450 examples processed\n",
      "\t\t\t 460 examples processed\n",
      "\t\t\t 470 examples processed\n",
      "\t\t\t 480 examples processed\n",
      "\t\t\t 490 examples processed\n",
      "\t\t\t 500 examples processed\n",
      "\t\t\t 510 examples processed\n",
      "\t\t\t 520 examples processed\n",
      "\t\t\t 530 examples processed\n",
      "\t\t\t 540 examples processed\n",
      "\t\t\t 550 examples processed\n",
      "\t\t\t 560 examples processed\n",
      "\t\t\t 570 examples processed\n",
      "\t\t\t 580 examples processed\n",
      "\t\t\t 590 examples processed\n",
      "\t\t\t 600 examples processed\n",
      "\t\t\t 610 examples processed\n",
      "\t\t\t 620 examples processed\n",
      "\t\t\t 630 examples processed\n",
      "\t\t\t 640 examples processed\n",
      "\t\t\t 650 examples processed\n",
      "\t\t\t 660 examples processed\n",
      "\t\t\t 670 examples processed\n",
      "\t\t\t 680 examples processed\n",
      "\t\t\t 690 examples processed\n",
      "\t\t\t 700 examples processed\n",
      "\t\t\t 710 examples processed\n",
      "\t\t\t 720 examples processed\n",
      "\t\t\t 730 examples processed\n",
      "\t\t\t 740 examples processed\n",
      "\t\t\t 750 examples processed\n",
      "\t\t\t 760 examples processed\n",
      "\t\t\t 770 examples processed\n",
      "\t\t\t 780 examples processed\n",
      "\t\t\t 790 examples processed\n",
      "\t\t\t 800 examples processed\n",
      "\t\t\t 810 examples processed\n",
      "\t\t\t 820 examples processed\n",
      "\t\t\t 830 examples processed\n",
      "\t\t\t 840 examples processed\n",
      "\t\t\t 850 examples processed\n",
      "\t\t\t 860 examples processed\n",
      "\t\t\t 870 examples processed\n",
      "\t\t\t 880 examples processed\n",
      "\t\t\t 890 examples processed\n",
      "\t\t\t 900 examples processed\n",
      "\t\t\t 910 examples processed\n",
      "\t\t\t 920 examples processed\n",
      "\t\t\t 930 examples processed\n",
      "\t\t\t 940 examples processed\n",
      "\t\t\t 950 examples processed\n",
      "\t\t\t 960 examples processed\n",
      "\t\t\t 970 examples processed\n",
      "\t\t\t 980 examples processed\n",
      "\t\t\t 990 examples processed\n",
      "\t\t\t 1000 examples processed\n",
      "\t\t\t 1010 examples processed\n",
      "\t\t\t 1020 examples processed\n",
      "\t\t\t 1030 examples processed\n",
      "\t\t\t 1040 examples processed\n",
      "\t\t\t 1050 examples processed\n",
      "\t\t\t 1060 examples processed\n",
      "\t\t\t 1070 examples processed\n",
      "\t\t\t 1080 examples processed\n",
      "\t\t\t 1090 examples processed\n",
      "\t\t\t 1100 examples processed\n",
      "\t\t\t 1110 examples processed\n",
      "\t\t\t 1120 examples processed\n",
      "\t\t\t 1130 examples processed\n",
      "\t\t\t 1140 examples processed\n",
      "\t\t\t 1150 examples processed\n",
      "\t\t\t 1160 examples processed\n",
      "\t\t\t 1170 examples processed\n",
      "\t\t\t 1180 examples processed\n",
      "\t\t\t 1190 examples processed\n",
      "\t\t\t 1200 examples processed\n",
      "\t\t\t 1210 examples processed\n",
      "\t\t\t 1220 examples processed\n",
      "\t\t\t 1230 examples processed\n",
      "\t\t\t 1240 examples processed\n",
      "\t\t\t 1250 examples processed\n",
      "\t\t\t 1260 examples processed\n",
      "\t\t\t 1270 examples processed\n",
      "\t\t\t 1280 examples processed\n",
      "\t\t\t 1290 examples processed\n",
      "\t\t\t 1300 examples processed\n",
      "\t\t\t 1310 examples processed\n",
      "\t\t\t 1320 examples processed\n",
      "\t\t\t 1330 examples processed\n",
      "\t\t\t 1340 examples processed\n",
      "\t\t\t 1350 examples processed\n",
      "\t\t\t 1360 examples processed\n",
      "\t\t\t 1370 examples processed\n",
      "\t\t\t 1380 examples processed\n",
      "\t\t\t 1390 examples processed\n",
      "\t\t\t 1400 examples processed\n",
      "\t\t\t 1410 examples processed\n",
      "\t\t\t 1420 examples processed\n",
      "\t\t\t 1430 examples processed\n",
      "\t\t\t 1440 examples processed\n",
      "\t\t\t 1450 examples processed\n",
      "\t\t\t 1460 examples processed\n",
      "\t\t\t 1470 examples processed\n",
      "\t\t\t 1480 examples processed\n",
      "\t\t\t 1490 examples processed\n",
      "\t\t\t 1500 examples processed\n",
      "\t\t\t 1510 examples processed\n",
      "\t\t\t 1520 examples processed\n",
      "\t\t\t 1530 examples processed\n",
      "\t\t\t 1540 examples processed\n"
     ]
    }
   ],
   "source": [
    "## Forth step ##\n",
    "features = forth_step(training_set, IDs, node_info, stemmer, stpwds, g, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of lists into array\n",
    "# Documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "training_features = np.array(features).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "#training_features = preprocessing.scale(training_features)\n",
    "np.save(path_to_data + 'shortest_path_edge_connectivity_train.npy', training_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test features calculating.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_test= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forth step ##\n",
    "features_test = forth_step(testing_set, IDs, node_info, stemmer, stpwds, g, graph, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of lists into array\n",
    "# Documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "testing_features = np.array(features_test).T\n",
    "# Scale\n",
    "#testing_features = preprocessing.scale(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path_to_data + 'shortest_path_edge_connectivity_test.npy', testing_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
